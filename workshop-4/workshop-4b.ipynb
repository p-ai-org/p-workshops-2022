{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRpDRNB2aN7f"
   },
   "source": [
    "# P-ai AI/ML Workshop: Session 4b\n",
    "\n",
    "Welcome to P-ai's third session of the AI/ML workshop series! Today we'll learn about\n",
    "- Deep learning\n",
    "    - How to build and train a neural net with Tensorflow and Keras\n",
    "    - Types of neural nets\n",
    "- Solving a real-world classification problem with a neural net\n",
    "\n",
    "<img src=\"https://images.squarespace-cdn.com/content/5d5aca05ce74150001a5af3e/1580018583262-NKE94RECI46GRULKS152/Screen+Shot+2019-12-05+at+11.18.53+AM.png?content-type=image%2Fpng\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Intro to Tensorflow and Keras\n",
    "\n",
    "<img src=\"https://3.bp.blogspot.com/-QZVBl08fmPk/XhO909Ha1dI/AAAAAAAACZI/q1a1UykGKe0KDUZ_ZITtWmM7bBJFRrvPQCLcBGAsYHQ/s1600/tensorflowkeras.jpg\" width=\"500px\">\n",
    "\n",
    "You might be wondering how to actually build and train a neural net. The most popular frameworks for deep learning are Google's [Tensorflow](https://www.tensorflow.org/) and Facebook's [Pytorch](https://pytorch.org/). Under the hood, it's basically a bunch of optimized graph algorithms that are necessary for neural networks.\n",
    "\n",
    "While you can build a neural net with Tensorflow alone (and in the future, you might need to do this to create a more \"customized\" neural net), this can often be a bit more involved than it has to be for a beginner. Luckily, Google also developed [Keras](https://keras.io/), which is an API for Tensorflow; in other words, it lets you write more readable and intuitive code, and Keras takes care of the nitty-gritty Tensorflow-y details.\n",
    "\n",
    "Let's take a look at how we would build the hypothetical neural net above with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Imports; this might take a second to load '''\n",
    "import tensorflow\n",
    "# Import layers we need (in this case, just Dense)\n",
    "from tensorflow.keras.layers import Dense\n",
    "# Sequential model means you just add layers in a sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Adam optimizer\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1602469758619,
     "user": {
      "displayName": "Marcos Acosta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiYeZ8kTWsuWNznnsrAf2xPyKcWUCiOxCeuFoPX=s64",
      "userId": "11821412167263768095"
     },
     "user_tz": 240
    },
    "id": "ElTVlnuLa2oP",
    "outputId": "bff01053-1aa3-4a4c-88f4-c1c2263e26b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 14:54:14.593870: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "''' Build model '''\n",
    "model = Sequential()                                   # Define sequential model\n",
    "model.add(Dense(128, input_dim=26))                    # Input layer and first hidden layer\n",
    "model.add(Dense(256))                                  # Second hidden layer\n",
    "model.add(Dense(3, activation='softmax'))              # Output layer; note activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Choose optimizer and loss function '''\n",
    "opt = Adam(learning_rate=0.001)              # Set learning rate to 0.001\n",
    "loss = 'categorical_crossentropy'            # Using categorical crossentropy for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               3456      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 37,251\n",
      "Trainable params: 37,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' Compile model and print layer summary '''\n",
    "model.compile(optimizer=opt, \n",
    "    loss=loss,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's that! Now, let's imagine we have some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Generate enough random numbers\n",
    "random_numbers = np.random.rand(1000 * 26)\n",
    "# This represents 1,000 examples, each with 26 features (frequencies a-z)\n",
    "X = np.reshape(random_numbers, (1000, 26))\n",
    "# This represents 1,000 one-hot vectors (which language the corresponding x actually is)\n",
    "y = np.zeros((1000, 3), dtype=int)\n",
    "for i in range(len(y)):\n",
    "    y[i][random.randint(0, 2)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fake dataset:\")\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"First X:\", X[0])\n",
    "print(\"y shape: \", y.shape)\n",
    "print(\"First y:\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fit our model to this fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Here we can specify the number of epochs to train on, the batch size, and much more\n",
    "# Check out the documentation for more!\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We see a lot of stuff being output, and it might seem like a lot; here's the breakdown:\n",
    "\n",
    "<img src=\"images/tf_output.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how, as the model trained on the training data for more epochs, the loss decreased and the accuracy increased. This is to be expected; the more times the model sees the data, the more it can fit to that data. You should probably be concerned about something, though...\n",
    "\n",
    "Since the data is random, the model should have not much more than a 33% accuracy. Yet, we see at the end of 10 epochs, the model's accuracy is above 40%. How can this be? Did the model discover some hidden patterns in the apparently random data?\n",
    "\n",
    "No. This is the classic problem of **overfitting**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190523171258/overfitting_2.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virtually every machine learning model has the tendency to overfit on its training data; that is, it begins to learn the *specifics* of the data instead of the *general trend*. This isn't good, because that means your model is **unstable**, which means (among other things) it *won't generalize well*. This is why we have training and test sets; we test our model on data it's never seen before to test whether it can actually generalize what it's learned, or if it overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our model on the test dataset\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of `evaluate` is a list of the final loss and accuracy (or whatever metric we compiled the model with). We can see that the model is actually only right ~33% of the time on data it hasn't seen before, which is exactly what we would expect. That means that our model did overfit on the training data a bit, as we would expect would happen with random data.\n",
    "\n",
    "**Helpful tip**: When calling `.fit()`, you can also pass in a validation split, which splits your training data *again* into training and validation data, and Tensorflow will test your model on the validation data after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=26))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    opt = Adam(learning_rate=0.001)\n",
    "    loss = 'categorical_crossentropy'\n",
    "    model.compile(optimizer=opt, \n",
    "        loss=loss,\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = buildModel()\n",
    "# Use 20% of the data for validation\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the validation loss and accuracy after each epoch and, as we would expect, the validation loss / accuracy don't get any better the more we train.\n",
    "\n",
    "Before moving onto other types of neural nets and an example, we can quickly see how easy it is to make predictions with our neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data; shape: (1, 26)\n",
    "# That is, one example with 26 features\n",
    "dataToClassify = np.reshape(np.random.rand(26), (1,26))\n",
    "pred = model.predict(dataToClassify)\n",
    "# Print prediction of first (only) example\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that, for multiclass classification, the output of the model is a vector of probabilities for each class. To get a single class, we can easily find the index with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our model predicts that this hypothetical input (which we imagine is a vector of letter frequencies) belongs to language 0 (say, English).\n",
    "\n",
    "Pat yourself on the back, that's the basics of building a neural net!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3EV5_66d7Of"
   },
   "source": [
    "## 2. Other neural nets!\n",
    "\n",
    "There are many \"flavors\" of neural networks. Here are some of the most common:\n",
    "\n",
    "### Multilayer Perceptron (MLP) aka \"Vanilla\" Neural Net\n",
    "The neural nets we just looked at are MLPs, or the \"vanilla\" feed-forward neural net. Many times, people will refer to these \"basic\" neural nets as just \"neural nets\" because they're not specifying a more specific type. MLPs find a relationship between 1D input and 1D output. So, if you can encode both your inputs and outputs as vectors, chances are, all you need is an MLP.\n",
    "\n",
    "### Convolutional Neural Network (CNN)\n",
    "A CNN is a type of neural net that works best with input where the **location** of the values is important. Take our  vanilla neural net example; it doesn't matter which order we store the letter frequencies in our input vector so long as we stick with an order. We can't say the same for images; it may matter a lot *where* in an image some feature is. CNNs work by first **convolving** over the image, which would probably be a bit tangential to explain now, but you can read more about it [here](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/). In any case, convolution allows the CNN to learn features about the relative location of values in the matrix, which is perfect for image processing!\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg\" width=\"600px\">\n",
    "\n",
    "### Long Short Term Memory (LSTM)\n",
    "The LSTM is a type of **RNN** (Recurrent Neural Network), which means it's meant to deal with **temporal** data. For example, if you were trying to predict something about a **sequence** of values, a regular neural network wouldn't do the trick. The LSTM introduces mechanisms for the net to \"remember\" old information and synthesize it with new information (hence the name). For a long time, LSTMs were the go-to for learning language, since you can represent a series of words numerically (see [word embeddings](https://machinelearningmastery.com/what-are-word-embeddings/)!).\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*ahafyNt0Ph_J6Ed9_2hvdg.png\" width=\"600px\">\n",
    "\n",
    "### Transformer\n",
    "\n",
    "Transformers have really taken off in the past few years, especially with the enormous success of language models like BERT and GPT. Transformers build off of another type of model called the **encoder-decoder**, where the model learns both how to encode and decode data to get from input to output; the classic example is language translation. First, you encode a series of words in one language into a vector (the \"meaning\"), which is then decoded into a series of words in the second language. The transformer is basically an encoder-decoder on steroids; it has stacks of encoders and stacks of decoders, and it also implements this cool concept called [Attention](https://arxiv.org/abs/1706.03762), which helps the model learn *which* parts of the input correspond to which parts of the output. Transformers pretty much totally outperform LSTMs on translation tasks!\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png\" width=\"600px\">\n",
    "\n",
    "### GAN\n",
    "\n",
    "GAN stands for Generative Adversarial Network, and they were invented by machine learning celebrity [Ian Goodfellow](https://en.wikipedia.org/wiki/Ian_Goodfellow). The main idea of GANs is to have two components of the neural net; the *generator* and the *discriminator*, which \"compete\" with each other. The generator tries to generate data that is similar to the real data, and the discriminator tries to figure out which data are real and which were generated by the generator. Both \"adversaries\" get better throughout the training, and at some point, the generator may become so good, the discriminator can't tell which data is real and which is fake. At this point, thanks to the GAN, you have an impeccable generator and a classifier! There are lots of uses for GANs, from [generating images of people that don't exist](https://thispersondoesnotexist.com/) to [turning sketches into photorealistic images](https://arxiv.org/pdf/1801.02753.pdf).\n",
    "\n",
    "<img src=\"https://i1.wp.com/bdtechtalks.com/wp-content/uploads/2018/05/GANs.png?resize=696%2C304&ssl=1\" width=\"600px\">\n",
    "\n",
    "These are just a few of the most common \"types\" of models you'll hear about out there. There's no limit to the kinds of model you can put together, though. For example, if you need to learn spatio-temporal data, you can combine a CNN with an LSTM to get a CNN-LSTM. When you work with tensorflow directly, you can build your very own custom neural nets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Case Study\n",
    "\n",
    "To put our theory into practice, let's take a look at the [heart failure prediction dataset](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data). The goal of this task is to predict whether patients died from heart failure within a certain time frame after their last check-in. First, we should take a look at the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display data from file\n",
    "heart_data = pd.read_csv('data/heart_failure.csv')\n",
    "heart_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have 12 features and one prediction variable, `DEATH_EVENT`. Some of the variables are continuous (like `age`, `platelets`), and others are binary (e.g. `anaemia`, `diabetes`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTINUOUS_COLS = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
    "BINARY_COLS = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\n",
    "TARGET_COL = 'DEATH_EVENT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot all of our continuous variables against the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in CONTINUOUS_COLS:\n",
    "    # Distribution plot\n",
    "    sns.displot(heart_data, x=col, hue=TARGET_COL)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't immediately see that any of these features would be excellent predictors of heart failure. The `time` variable (the follow-up period) might offer the most clues; it seems that a very quick follow-up results in heart failure more often than a much later follow-up.\n",
    "\n",
    "We can also take a look at our binary variables; we'll plot them as pie graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in BINARY_COLS:\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    print(col)\n",
    "    ax1.pie(heart_data[heart_data[TARGET_COL] == 0][col].value_counts(), labels=[0,1], autopct='%1.1f%%')\n",
    "    ax1.set_title('Survived')\n",
    "    ax2.pie(heart_data[heart_data[TARGET_COL] == 1][col].value_counts(), labels=[0,1], autopct='%1.1f%%')\n",
    "    ax2.set_title('Died')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the board, we see little differences in the target variable due to any of these binary variables alone. Thus, we might expect this classification task be a bit challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X data is everything but the target column\n",
    "X_data = heart_data[heart_data.columns[:-1]]\n",
    "# y data is the target column\n",
    "y_data = heart_data[TARGET_COL]\n",
    "\n",
    "# Split into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "print(f\"{len(X_train)} training examples and {len(X_test)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also worth noting that the *scale* of the continuous variables vary drastically. Take a look at this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max age:\", max(X_data['age']))\n",
    "print(\"Max platelets:\", max(X_data['platelets']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is generally not ideal for neural nets. A big difference in scale between different features means that the \"larger scale\" features will naturally overpower the \"smaller scale\" ones, and the net will need to learn extremely small / large weights to keep the features comparable. This is unnecessary work for the neural net when we can just **normalize** our input first.\n",
    "\n",
    "There's a few different ways to normalize data, but the goal is to scale each features so they're comparable. We can use the [standard scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to perform standard normalization, which means transforming the data so that the mean and standard deviation of each feature is 0 and 1, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalify(data, columns, scaler=None):\n",
    "    ''' Apply normalization to the specified columns in data. Fits a scaler if one is not given '''\n",
    "    # Get columns to be normalized\n",
    "    data_to_normalize = data[columns]\n",
    "    # Get remaining data (not to be normalized)\n",
    "    remaining_cols = [c for c in data.columns if c not in columns]\n",
    "    remaining_data = data[remaining_cols]\n",
    "    # If no scaler is given, fit one\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data_to_normalize)\n",
    "    # Apply standard scaler\n",
    "    data_normalized = scaler.transform(data_to_normalize)\n",
    "    # Recombine normalized and remaining data, and also return the scaler\n",
    "    return np.hstack((data_normalized, remaining_data)), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply our normalization process\n",
    "X_train_normalized, scaler = normalify(X_train, CONTINUOUS_COLS)\n",
    "X_test_normalized, _ = normalify(X_test, CONTINUOUS_COLS, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For consistency, we'll also turn our y data into numpy arrays\n",
    "y_train, y_test = np.array(y_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our X data is now a matrix of values that mostly sit between -1 and 1. Cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X training data:\\n\", X_train_normalized)\n",
    "print(\"Shape:\", X_train_normalized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as expected, our y data are binary 0s (no heart failure) and 1s (heart failure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y training data:\\n\", y_train)\n",
    "print(\"Shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a model! We'll incorporate something called `Dropout`, which randomly deactivates (sets to 0) a certain proportion of weights in a layer. This might seem destructive; why do we want to un-learn what we've learned? Dropout is a great way to combat overfitting! \n",
    "\n",
    "By deactivating weights, the model is forced to re-learn those connections from the existing weights and in doing so, learns the data more deeply instead of \"memorizing\" the input-output relationship in the training data. You can think of dropout like working out; your muscles get damaged to grow back stronger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineBaselineModel():\n",
    "    ''' Define and return a neural net '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=12))         # Input layer has 12 features; second layer of 64 neurons\n",
    "    model.add(Dropout(0.3))                    # 30% of weights are deactivated\n",
    "    model.add(Dense(128))                      # Third layer has 128 neurons\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256))                      # Fourth layer has 256 neurons\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # One neuron for output with sigmoid activation\n",
    "    opt = Adam(learning_rate=0.01)             # Define optimizer\n",
    "    loss = 'binary_crossentropy'               # Binary crossentropy for loss\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = defineBaselineModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on data!\n",
    "model.fit(X_train_normalized, y_train, batch_size=8, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And evaluate!\n",
    "acc = model.evaluate(X_test_normalized, y_test)\n",
    "print(\"Accuracy:\", acc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance will vary every time you train the model, but when I ran it, the test accuracy was 78%. Let's check how well the model would do if it guessed `0` every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (sum(y_data) / len(y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, that's a little bit reassuring; the model performs about 10% better than it would by just guessing that there is never a heart failure. This isn't stellar, but if we preprocessed the data better (e.g. apply a log scale to `creatinine_phosphokinase`), or if we had more data, we could hope for a better performance.\n",
    "\n",
    "Even though we split our data into training and test data, it's very possible that, by random chance, our test set was \"easy\" and resulted in a better test accuracy than we would get with a different train-test split. For this reason, we often use **K-fold cross validation** as a way of combating this. Basically, the data gets split up into `k` \"folds\", and then the model is trained and tested on different combinations of those folds (each fold gets to be the test set once). We can take the average of each test accuracy as a more honest evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def create_data_splits(data, k=5):\n",
    "    ''' Returns a list of k tuples (X_train, X_test, y_train, y_test) '''\n",
    "    splits = []\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    # Get raw X and y data\n",
    "    X_data = data[data.columns[:-1]]\n",
    "    y_data = data[data.columns[-1]]\n",
    "    # kf.split returns train and test indexes\n",
    "    for train_indexes, test_indexes in kf.split(X_data):\n",
    "        # Get actual data by \"filtering\" by index using pandas' iloc\n",
    "        X_train, X_test = X_data.iloc[train_indexes], X_data.iloc[test_indexes]\n",
    "        y_train, y_test = y_data.iloc[train_indexes], y_data.iloc[test_indexes]\n",
    "        # Apply normalization\n",
    "        X_train_normalized, scaler = normalify(X_train, CONTINUOUS_COLS)\n",
    "        X_test_normalized, _ = normalify(X_test, CONTINUOUS_COLS, scaler=scaler)\n",
    "        y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "        # Add tuple of data to splits\n",
    "        splits.append((X_train_normalized, X_test_normalized, y_train, y_test))\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 10-fold validation\n",
    "k = 10\n",
    "splits = create_data_splits(heart_data, k=k)\n",
    "accuracies = []\n",
    "for X_train, X_test, y_train, y_test in splits:\n",
    "    # Important: define a new model every time you train on a new data split!\n",
    "    model = defineBaselineModel()\n",
    "    # verbose=0 doesn't show the progress bar\n",
    "    model.fit(X_train, y_train, batch_size=8, epochs=10, verbose=0)\n",
    "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    accuracies.append(accuracy)\n",
    "# Print average accuracy\n",
    "print(sum(accuracies) / k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this code, it resulted in an average accuracy around 80%, which isn't half bad, and substantially better than random guess.\n",
    "\n",
    "That's all we have time for today! Another great thing to check out would be a confusion matrix, which will let you visualize *how* the model misclassifies. Check out workshop 3 if you'd like a refresher on how to make / interpret those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing remarks\n",
    "\n",
    "<img src=\"working_on_workshop.jpg\" width=\"400px\">\n",
    "<br />\n",
    "<div width=\"100%\" style=\"text-align: center\">\n",
    "    Me working on this workshop at midnight; hope you've enjoyed learning about machine learning!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Session4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2fc80e121636b772365a55387a8486b641440f139290df9d34a00bf7086ae22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
