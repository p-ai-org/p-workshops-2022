{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac59a70",
   "metadata": {},
   "source": [
    "# P-ai AI/ML Workshop: Session 3\n",
    "\n",
    "Welcome to P-ai's third session of the AI/ML workshop series! Today we'll learn about\n",
    "- Gradient descent\n",
    "- A host of new machine learning algorithms to try out\n",
    "- How to use Git and GitHub\n",
    "\n",
    "<img src=\"https://images.squarespace-cdn.com/content/5d5aca05ce74150001a5af3e/1580018583262-NKE94RECI46GRULKS152/Screen+Shot+2019-12-05+at+11.18.53+AM.png?content-type=image%2Fpng\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08409f05",
   "metadata": {},
   "source": [
    "## 0. Session 2 Review\n",
    "\n",
    "Here are some key points from last week's session:\n",
    "- Supervised learning, unsupervised learning, reinforcement learning\n",
    "    - Supervised: have labeled data, want model to predict $y$ from $X$ (e.g. predict house price from picture)\n",
    "    - Unsupervised: don't have labeled data, want to learn patterns directly in data (e.g. clustering)\n",
    "    - Reinforcement: want to train agent to perform actions in an environment that optimize some reward (e.g. play Mario)\n",
    "- Linear regression\n",
    "    - Used to predict a value from ≥1 input variable(s)\n",
    "    - Expects linear relationship between input and output\n",
    "- Logistic regression\n",
    "    - Used to predict a binary class (0 or 1) from ≥1 input variable(s)\n",
    "    - Finds linear decision boundary to separate data from different classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f77cab3",
   "metadata": {},
   "source": [
    "## 1. Gradient descent\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Chocolate_Hills_overview.JPG/1200px-Chocolate_Hills_overview.JPG\" width=\"500px\">\n",
    "\n",
    "What's gradient descent, and why are we talking about it now? This is actually one answer to a question that was posed in the previous two workshops, but which we didn't have time to answer in detail:\n",
    "\n",
    "> *How* do machine learning models find the optimal parameters?\n",
    "\n",
    "There's an asterisk here, because not all machine learning algorithms use gradient descent, like decision trees (which we'll cover later today). That being said, it's pretty pervasive in machine learning and absolutely crucial in deep learning. Let's get started!\n",
    "\n",
    "Imagine you're blind, and you're standing on the side of a hill. You want to go downward- how do you know which way to go? You can feel with your feet *which direction the slope is*, and then go in the opposite direction of the upward slope. You then *walk some distance* in that direction, and *repeat the process*. That's pretty much it! Gradient descent is a fairly intuitive process.\n",
    "\n",
    "> So what do the hills represent?\n",
    "\n",
    "The hills represent **loss** (reminder: loss is a synonym for cost; the thing you want to minimize). If we're sticking with three dimensions for the sake of visualizing, that means that if you pick two coordinates (say, $x$ and $y$), there's a height (loss) associated with those coordinates. Also recall that in the case of 2D linear regression, we want to find a **weight** and a **bias**; let's use $w$ and $b$ as our variables. Also, we'll call the loss $J$ (it's just convention). That means that we want to find:\n",
    "$$\n",
    "J(w, b)_{min}\n",
    "$$\n",
    "\n",
    "**\\[WARNING\\] CALCULUS INCOMING**  \n",
    "(If you're not comfortable with calculus, don't worry about this next stuff– but if you want to get as concrete as possible, this is for you)\n",
    "\n",
    "Mathematically, slope can be found by taking the *derivative* of a function. When you take the partial derivative of a function in all directions, that's called the *gradient*. The gradient of a function is a *vector* that points in the **direction of steepest ascent**. So, to go *down the gradient*, we would ideally wish to find the gradient of the loss function with respect to the weights, then take the *negative of the gradient* (direction of steepest descent), and update the weights in that direction.\n",
    "\n",
    "Here's that same statement, written more in more math-y terms:\n",
    "$$\n",
    "w \\leftarrow w - \\alpha \\nabla J\n",
    "$$\n",
    "\n",
    "here $\\leftarrow$ means \"set equal to\", $\\nabla$ is the symbol for taking the gradient, and alpha ($\\alpha$) is what's called the **learning rate**. The learning rate decides *how much* to update the weight based on the gradient. Let's try to visualize this a little better; here's an example where the loss ($J$) is a function of only one variable:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg\" width=\"400px\">\n",
    "\n",
    "Okay! Enough of that for now, let's summarize what all of this was about:\n",
    "\n",
    "- Loss is a function of the model parameters, like weights and biases\n",
    "    - That is, there are certain combinations of parameters that minimize loss $\\approx$ maximize \"learning\"\n",
    "- Gradient is like slope, but it has a direction (always points in the direction of steepest ascent)\n",
    "- To find a minimum (global, hopefully, but not necessarily), we \"descend\" the \"gradient\"\n",
    "- At each step, the model makes a prediction and uses the difference between the predicted and actual value to estimate the gradient\n",
    "- The weights are then updated accordingly to incrementally decrease the loss, and the process repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d09e50",
   "metadata": {},
   "source": [
    "## Taking a deeper look at gradient descent\n",
    "\n",
    "### The simplified case\n",
    "\n",
    "A couple sessions ago we talked about methods to quantify error in our predictions, the cost function. We opted to use mean squared error, defined to be:\n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^n (\\hat{Y} - Y)^2$$\n",
    "\n",
    "We mentioned that MSE is useful as the power makes the derivative meaningful. Now we'll see why that matters!\n",
    "\n",
    "Let's look at the simple 2D case first. We'll call back to the regression case of $$\\hat{y} = wx + b$$. Remember we our looking for some w and some b that minimizes our error. Let's imagine our error can be quantified as some quadratic function which we'll approximate to be $$J(\\tilde{Y}) = \\tilde{Y}^2$$. Note that this is an unrealistic example. When we perform machine learning, our error usually cannot be quantified by a simple function, and even if we could, we don't know it in advance. However, let's just play with this for a second.\n",
    "\n",
    "This is basically a parabolic function. Visually, it looks like this:\n",
    "\n",
    "<img src = https://miro.medium.com/max/720/1*wOcqaaLlNo7X56PJ-lYFqQ.png>\n",
    "\n",
    "We are trying to make it to the local minima. How do we find that point with our current architecture? Computing the derivative.\n",
    "\n",
    "$$\\frac{d}{d\\tilde{Y}} J(\\tilde{Y})$$\n",
    "\n",
    "This basically defines the steepness of the parabola at some arbitrary point. How do we find the minima? Notice at the base of the parabola where error is lowest, the slope is actually zero! So we can take:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\tilde{Y}} J(\\tilde{Y}) = \\frac{d}{d\\tilde{Y}} \\tilde{Y}^2 = 2\\tilde{Y} = 0\n",
    "$$\n",
    "\n",
    "Solving this, we see that for a parabola centered at the point (0, 0), the point of minimum error is (0, 0).\n",
    "\n",
    "Let's think about this from the gradient descent point of view. How would we be able to arrive at (0, 0) by going through a bunch of trials? Consider this parabola shape again. Let's hypothesize that our error is in this parabolic shape, but we DON'T KNOW IT! \n",
    "\n",
    "#### Discussion Question\n",
    "What can we do to figure it out? What is the derivative telling us?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda95ffa",
   "metadata": {},
   "source": [
    "### Concerning Problems\n",
    "However, life is not so simple... Let's look at the following 2D error function\n",
    "\n",
    "<img src= images/false_error.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c20a28",
   "metadata": {},
   "source": [
    "Note that there are two local minima in this plot... Why might this be troubling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8b56b",
   "metadata": {},
   "source": [
    "#### Getting Unstuck: Some Examples\n",
    "\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "    - Injects some randomness\n",
    "- Momentum\n",
    "    - Aggregates our gradient: analogous to physical concept of momentum\n",
    "\n",
    "We'll talk about these later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd383b4d",
   "metadata": {},
   "source": [
    "### The Learning Rate Question:\n",
    "\n",
    "How quickly we move down the curve is dictated by a parameter we can set called the learning rate. Let's think about two cases:\n",
    "- Learning rate is too big\n",
    "- Learning rate is too small\n",
    "\n",
    "<img src = https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png width = 800px>\n",
    "\n",
    "- You can also set an adaptive learning rate that can move based off your iteration number\n",
    "- This can have its own problems - why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e24d9",
   "metadata": {},
   "source": [
    "### The 3D Case\n",
    "\n",
    "- Its a similar story but our mathematical framework must slightly change\n",
    "- Now need to figure out the slope in 3-D... or, the gradient $$\\nabla$$\n",
    "\n",
    "Think of it as a replacement of the 2-D slope with a vector that describes the direction and rate you are headed down the hill\n",
    "\n",
    "This is an increase in complexity! Wandering to the correct minima might be a bit harder now... why is that?\n",
    "\n",
    "<img src = https://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/3d-gradient-cos.svg/525px-3d-gradient-cos.svg.png width = 400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68efc3a2",
   "metadata": {},
   "source": [
    "We will encounter the same problem as the 2-D case... misleading local minima:\n",
    "\n",
    "<img src=\"https://www.fromthegenesis.com/wp-content/uploads/2018/06/Gradie_Desce.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa2051",
   "metadata": {},
   "source": [
    "### Getting out: Stochastic Gradient Descent\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc22371f3870926dbbcdf04161196d7334d8dc09d0b069ea32b843c425bf39e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
