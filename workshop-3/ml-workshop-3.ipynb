{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac59a70",
   "metadata": {},
   "source": [
    "# P-ai AI/ML Workshop: Session 3\n",
    "\n",
    "Welcome to P-ai's third session of the AI/ML workshop series! Today we'll learn about\n",
    "- Gradient descent\n",
    "- A host of new machine learning algorithms to try out\n",
    "- How to use Git and GitHub\n",
    "\n",
    "<img src=\"https://images.squarespace-cdn.com/content/5d5aca05ce74150001a5af3e/1580018583262-NKE94RECI46GRULKS152/Screen+Shot+2019-12-05+at+11.18.53+AM.png?content-type=image%2Fpng\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08409f05",
   "metadata": {},
   "source": [
    "## 0. Session 2 Review\n",
    "\n",
    "Here are some key points from last week's session:\n",
    "- Supervised learning, unsupervised learning, reinforcement learning\n",
    "    - Supervised: have labeled data, want model to predict $y$ from $X$ (e.g. predict house price from picture)\n",
    "    - Unsupervised: don't have labeled data, want to learn patterns directly in data (e.g. clustering)\n",
    "    - Reinforcement: want to train agent to perform actions in an environment that optimize some reward (e.g. play Mario)\n",
    "- Linear regression\n",
    "    - Used to predict a value from ≥1 input variable(s)\n",
    "    - Expects linear relationship between input and output\n",
    "- Logistic regression\n",
    "    - Used to predict a binary class (0 or 1) from ≥1 input variable(s)\n",
    "    - Finds linear decision boundary to separate data from different classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f77cab3",
   "metadata": {},
   "source": [
    "## 1. Gradient descent\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Chocolate_Hills_overview.JPG/1200px-Chocolate_Hills_overview.JPG\" width=\"500px\">\n",
    "\n",
    "What's gradient descent, and why are we talking about it now? This is actually one answer to a question that was posed in the previous two workshops, but which we didn't have time to answer in detail:\n",
    "\n",
    "> *How* do machine learning models find the optimal parameters?\n",
    "\n",
    "There's an asterisk here, because not all machine learning algorithms use gradient descent, like decision trees (which we'll cover later today). That being said, it's pretty pervasive in machine learning and absolutely crucial in deep learning. Let's get started!\n",
    "\n",
    "Imagine you're blind, and you're standing on the side of a hill. You want to go downward- how do you know which way to go? You can feel with your feet *which direction the slope is*, and then go in the opposite direction of the upward slope. You then *walk some distance* in that direction, and *repeat the process*. That's pretty much it! Gradient descent is a fairly intuitive process.\n",
    "\n",
    "> So what do the hills represent?\n",
    "\n",
    "The hills represent **loss** (reminder: loss is a synonym for cost; the thing you want to minimize). If we're sticking with three dimensions for the sake of visualizing, that means that if you pick two coordinates (say, $x$ and $y$), there's a height (loss) associated with those coordinates. Also recall that in the case of 2D linear regression, we want to find a **weight** and a **bias**; let's use $w$ and $b$ as our variables. Also, we'll call the loss $J$ (it's just convention). That means that we want to find:\n",
    "Let's try to visualize this a little better; here's an example where the loss ($J$) is a function of only one variable:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg\" width=\"400px\">\n",
    "\n",
    "The learning step in the image above is tied to the learning rate, which decides *how much* to update the weight based on the gradient. \n",
    "\n",
    "Okay! Enough of that for now, let's summarize what all of this was about:\n",
    "\n",
    "- Loss is a function of the model parameters, like weights and biases\n",
    "    - That is, there are certain combinations of parameters that minimize loss $\\approx$ maximize \"learning\"\n",
    "- Gradient is like slope, but it has a direction (always points in the direction of steepest ascent)\n",
    "- To find a minimum (global, hopefully, but not necessarily), we \"descend\" the \"gradient\"\n",
    "- At each step, the model makes a prediction and uses the difference between the predicted and actual value to estimate the gradient\n",
    "- The weights are then updated accordingly to incrementally decrease the loss, and the process repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d09e50",
   "metadata": {},
   "source": [
    "## Taking a deeper look at gradient descent\n",
    "\n",
    "### The simplified case\n",
    "\n",
    "A couple sessions ago we talked about methods to quantify error in our predictions, the cost function. We opted to use mean squared error, defined to be:\n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^n (\\hat{Y} - Y)^2$$\n",
    "\n",
    "We mentioned that MSE is useful as the power makes the derivative meaningful. Now we'll see why that matters!\n",
    "\n",
    "Let's look at the simple 2D case first. We'll call back to the regression case of $$\\hat{y} = wx + b$$. Remember we our looking for some w and some b that minimizes our error. Let's imagine our error can be quantified as some quadratic function which we'll approximate to be $$J(\\tilde{Y}) = \\tilde{Y}^2$$. Note that this is an unrealistic example. When we perform machine learning, our error usually cannot be quantified by a simple function, and even if we could, we don't know it in advance. However, let's just play with this for a second.\n",
    "\n",
    "This is basically a parabolic function. Visually, it looks like this:\n",
    "\n",
    "<img src = https://miro.medium.com/max/720/1*wOcqaaLlNo7X56PJ-lYFqQ.png>\n",
    "\n",
    "We are trying to make it to the local minima. How do we find that point with our current architecture? Computing the derivative.\n",
    "\n",
    "$$\\frac{d}{d\\tilde{Y}} J(\\tilde{Y})$$\n",
    "\n",
    "This basically defines the steepness of the parabola at some arbitrary point. How do we find the minima? Notice at the base of the parabola where error is lowest, the slope is actually zero! So we can take:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\tilde{Y}} J(\\tilde{Y}) = \\frac{d}{d\\tilde{Y}} \\tilde{Y}^2 = 2\\tilde{Y} = 0\n",
    "$$\n",
    "\n",
    "Solving this, we see that for a parabola centered at the point (0, 0), the point of minimum error is (0, 0).\n",
    "\n",
    "Let's think about this from the gradient descent point of view. How would we be able to arrive at (0, 0) by going through a bunch of trials? Consider this parabola shape again. Let's hypothesize that our error is in this parabolic shape, but we DON'T KNOW IT! \n",
    "\n",
    "#### Discussion Question\n",
    "What can we do to figure it out? What is the derivative telling us? \n",
    "\n",
    "The process:\n",
    "- Initialize the model with some set of weights and biases\n",
    "- Test the model, how well does it perform, what is the error?\n",
    "- Based off the performance, how can we tune the weights and biases to reduce error?\n",
    "\n",
    "What does this look like mathematically?\n",
    "$$\\Delta w = \\frac{\\partial{J}}{\\partial{w}}$$\n",
    "\n",
    "In English... Updating our weights results in some change in error that directly arises from us modifying our weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda95ffa",
   "metadata": {},
   "source": [
    "### Concerning Problems\n",
    "However, life is not so simple... Let's look at the following 2D error function\n",
    "\n",
    "<img src= images/false_error.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c20a28",
   "metadata": {},
   "source": [
    "Note that there are two local minima in this plot... Why might this be troubling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8b56b",
   "metadata": {},
   "source": [
    "#### Getting Unstuck: Some Examples\n",
    "\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "    - Injects some randomness\n",
    "- Momentum\n",
    "    - Aggregates our gradient: analogous to physical concept of momentum\n",
    "\n",
    "We'll talk about these later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd383b4d",
   "metadata": {},
   "source": [
    "### The Learning Rate Question:\n",
    "\n",
    "How quickly we move down the curve is dictated by a parameter we can set called the learning rate. Let's think about two cases:\n",
    "- Learning rate is too big\n",
    "- Learning rate is too small\n",
    "\n",
    "What does the learning rate look like mathematically? Simple! It just scales our derivative, aka it can modify how big or small the step we decide to take is\n",
    "\n",
    "$$\\Delta w = \\eta * \\frac{\\partial{J}}{\\partial{w}}$$\n",
    "\n",
    "<img src = https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png width = 800px>\n",
    "\n",
    "- You can also set an adaptive learning rate that can move based off your iteration number\n",
    "- This can have its own problems - why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e24d9",
   "metadata": {},
   "source": [
    "### The 3D Case\n",
    "\n",
    "- Its a similar story but our mathematical framework must slightly change\n",
    "- Now need to figure out the slope in 3-D... or, the gradient, the weird little upside down triangle thing $$\\nabla$$\n",
    "\n",
    "Think of it as a replacement of the 2-D slope with a vector that describes the direction and rate you are headed down the hill\n",
    "\n",
    "This is an increase in complexity! Wandering to the correct minima might be a bit harder now... why is that?\n",
    "\n",
    "<img src = https://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/3d-gradient-cos.svg/525px-3d-gradient-cos.svg.png width = 400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68efc3a2",
   "metadata": {},
   "source": [
    "We will encounter the same problem as the 2-D case... misleading local minima:\n",
    "\n",
    "<img src=\"https://www.fromthegenesis.com/wp-content/uploads/2018/06/Gradie_Desce.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa2051",
   "metadata": {},
   "source": [
    "### Getting out: Stochastic Gradient Descent\n",
    "\n",
    "#### What is a stochastic process? \n",
    "Basically, randomness..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901eaace",
   "metadata": {},
   "source": [
    "<img src=\"images//brownian_motion.gif\" width = 800px>\n",
    "\n",
    "(Figure generated with code from Vladimir Ilievski)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d719a3",
   "metadata": {},
   "source": [
    "In typical gradient descent, we compute the derivative for every prediction... However, how does this scale? Discussion time\n",
    "\n",
    "What SGD does is it randomly samples a set of data points to compute derivatives from\n",
    "\n",
    "How does this help with the local minima problem? Let's revisit the above graph. How might a bit of randomness and skew from random observations help us escape local minima?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3231c22e",
   "metadata": {},
   "source": [
    "#### A Visual Comparison\n",
    "\n",
    "<img src = \"https://media.geeksforgeeks.org/wp-content/uploads/gdp.png\" width = 400px > \n",
    "<figcaption>Gradient Descent Learning Path</figcaption> \n",
    "<img src = \"https://media.geeksforgeeks.org/wp-content/uploads/sgd-1.jpg\" width = 400px>\n",
    "<figcaption>SGD Learning Path</figcaption>\n",
    "\n",
    "(images from geeksforgeeks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5002b1d",
   "metadata": {},
   "source": [
    "#### Momentum\n",
    "\n",
    "In physics, if an object is moving, it has some momentum! What does this mean? It wants to keep going\n",
    "\n",
    "How does this apply to neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a898e",
   "metadata": {},
   "source": [
    "<img src = \"https://visualstudiomagazine.com/articles/2017/08/01/~/media/ECG/visualstudiomagazine/Images/2017/08/0817vsm_McCaffreyFig1.ashx\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e3708b",
   "metadata": {},
   "source": [
    "Putting aside this math jargon, what does this mean? The second term in the equation with momentum basically allows us to take into consideration how we were optimizing in our previous round of trials. Let's think about this in terms of simple directions.\n",
    "\n",
    "Let's say in our last run, we moved \"right\"\n",
    "- In our current run, we now decide to move left, the magnitude of our left movement will be diminished by momentum\n",
    "- In our current run, we now decide to move right, the magnitude of our right movement will be increased by momentum\n",
    "\n",
    "How can this help with the local minima problem? Discussion time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e1142",
   "metadata": {},
   "source": [
    "#### Some more concerns\n",
    "\n",
    "Let's look at some different error landscapes and chat about them\n",
    "\n",
    "<img src = \"http://www.offconvex.org/assets/saddle/minmaxsaddle.png\">\n",
    "\n",
    "<img src = http://www.offconvex.org/assets/saddle/symmetrysmall.png>\n",
    "\n",
    "<img src = http://www.offconvex.org/assets/saddle/highorder.png>\n",
    "\n",
    "(An excellent article, if you are curious) http://www.offconvex.org/2016/03/22/saddlepoints/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268e1db",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c9d00ddaa79aa68c3a541f4a79b3085c4c0d74dc6caab7ebd7301921f5b4803"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
